{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Cloning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I will consider the rubric points individually and describe how I addressed each point in my implementation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Submitted & Code Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Are all required files submitted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My project includes the following files:\n",
    "\n",
    "* `model.py` containing the script to create and train the model\n",
    "* `drive.py` for driving the car in autonomous mode\n",
    "* `model.h5` containing a trained convolution neural network\n",
    "* `CarND-Behavioral-Cloning-Writeup.md` and `CarND-Behavioral-Cloning-Writeup.ipynb` summarizing the results (you're reading it now!)\n",
    "* `video.mp4` with the recorded track behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Is the code functional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `python drive.py model.h5` to steer the car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Is the code usable and readable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.py` is the main entry point to the project:\n",
    "\n",
    "* function `load_lines` reads the lines from the simulator output (driving log)\n",
    "* function `generator` uses Python generator to generate input samples for training and validation, because the amount of data is too large to fit in memory\n",
    "    * it uses `translate_image` and `flip_image` explained later\n",
    "* function `load_generator` calls `load_lines` and `generator` to prepare the data for trainign\n",
    "* function `steering_model` defines a keras model I have used. It also optionally prints the network structure.\n",
    "* function `fit_model` calls the `steering_model` and `load_generator` to perform `fit_generator` operation and fit the neural network using Adam optimizer\n",
    "* function `visualize_model` the training history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Has an appropriate model architecture been employed for the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point for the model was the architecture described in `End to End Learning for Self-Driving Cars`. Apart from the changed below I have also adjusted parameters of this architecture (layer sizes and depths) as an iterative process during training.\n",
    "\n",
    "* The model contains convolutional, pooling and dense (with ReLu activation) trainable layers.\n",
    "* I have added rescaling Keras Labmda layer to normalize the input with GPU parallelization\n",
    "* I have added Cropping2D to focus the network on the road and reduce number of parameters by 2x\n",
    "* I have added Dropout layers to combat overfitting\n",
    "\n",
    "Below you can see the full description of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 88, 318, 64)       1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 88, 318, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 86, 316, 32)       18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 86, 316, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 84, 314, 16)       4624      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 84, 314, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 82, 312, 8)        1160      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 82, 312, 8)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 41, 156, 8)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 41, 156, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51168)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                818704    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 845,305\n",
      "Trainable params: 845,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7f18086a47f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import steering_model\n",
    "steering_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Has an attempt been made to reduce overfitting of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have employed following strategies to reduce overfitting:\n",
    "* The model contains dropout layers\n",
    "* The model was trained and validated on different data sets\n",
    "* The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track\n",
    "* Dataset was augmented with small affine translations (`translate_image`), mirror images (`flip_image`) and multiple camera adjustments to minimize the reliance on the track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Have the model parameters been tuned appropriately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used an adam optimizer, so the learning rate was not tuned manually.\n",
    "\n",
    "The parameter I have experimented with is the multiple camera correction. `0.25` gave me visibly better results than `0.2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Is the training data chosen appropriately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data was chosen to keep the vehicle driving on the road. I have used the dataset provided in the lecture resources, but I augmented it in the following ways.\n",
    "\n",
    "* Usage of multiple cameras with 0.25 correction to ensure recovery from deviations.\n",
    "* Horizontal flipping of images to combat the steer-left nature of the track. (`flip_image`). Every sample can be flipped or not with probability 0.5.\n",
    "* Random small affine transformations help generalize the training input (`translate_image`). Every time I draw a sample from a generator I translate the image a little and adjust the angle to make up for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture and Training Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Is the solution design documented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall strategy for deriving a model architecture was to make sure the model generalizes the image input (hence the usage of convolutional and maxpooling layers).\n",
    "\n",
    "The starting point for the model was the architecture described in `End to End Learning for Self-Driving Cars`. Although I have changed the architecture somewhat most of the improvements came from the data augmentation. The design choices have been described in the cells above.\n",
    "\n",
    "At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Is the model architecture documented?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above you can see the model architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Is the creation of the training dataset and training process documented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import load_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['center', 'left', 'right', 'steering', 'throttle', 'brake', 'speed']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8036"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = load_lines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used the dataset provided in the lecture resources. It contained `8036` intial samples. I have augmented those with the use of multiple cameras (3x) and image flipping (2x), which yielded `8036 * 2 * 3 = 48 216` potential samples. I have used a generator with batch 16 to randomly sample those images. \n",
    "\n",
    "As described above I have also used random translations on every image to make the model generalize outside the training situation.\n",
    "\n",
    "The data points have been randomly shuffled the data set and 20$ of the data was put into a validation set.\n",
    "\n",
    "This amount of data resulted in memory issues so I needed to use generators to feed the batches to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Is the car able to navigate correctly on test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enclose `video.py` file to show how the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"video.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(\"video.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
